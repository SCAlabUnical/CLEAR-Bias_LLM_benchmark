{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install openpyxl\n",
        "!pip install openai\n",
        "!pip install google-generativeai\n",
        "!pip install pip install google-api-core\n",
        "!pip install grpcio"
      ],
      "metadata": {
        "id": "bXXcndD6dbs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import google.generativeai as genai\n",
        "from google.api_core import exceptions\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "import json\n",
        "\n",
        "client_db = OpenAI(api_key=\"\", base_url=\"https://api.deepbricks.ai/v1/\")\n",
        "client_ds = OpenAI(api_key=\"\", base_url=\"https://api.deepseek.com\")\n",
        "genai.configure(api_key=\"\")"
      ],
      "metadata": {
        "id": "zTupf1jIkKTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm-s7uFudKJy"
      },
      "outputs": [],
      "source": [
        "def wait_with_exponential_backoff(retries, maximum_backoff=64):\n",
        "    delay = min((2 ** retries) + random.uniform(0, 1), maximum_backoff)\n",
        "    print(f\"Waiting for {delay:.2f} seconds before the next attempt...\")\n",
        "    time.sleep(delay)\n",
        "\n",
        "\n",
        "def call_api_with_backoff(api_call, *args, **kwargs):\n",
        "    retries = 0\n",
        "    maximum_backoff = 64\n",
        "    while True:\n",
        "        try:\n",
        "            return api_call(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error: {str(e)}. Applying exponential backoff...\")\n",
        "            wait_with_exponential_backoff(retries, maximum_backoff)\n",
        "            retries += 1\n",
        "            raise e\n",
        "\n",
        "\n",
        "def invoke_db(model_name, prompt):\n",
        "    def api_call():\n",
        "        return client_db.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "    completion = call_api_with_backoff(api_call)\n",
        "    return completion.choices[0].message.content\n",
        "\n",
        "\n",
        "def invoke_ds(prompt):\n",
        "    def api_call():\n",
        "        return client_ds.chat.completions.create(\n",
        "            model=\"deepseek-chat\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            stream=False\n",
        "        )\n",
        "    response = call_api_with_backoff(api_call)\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "def invoke_gemini(prompt):\n",
        "    def api_call():\n",
        "        model = genai.GenerativeModel(model_name=\"gemini-2.0-flash\")\n",
        "        chat_session = model.start_chat(history=[])\n",
        "        response = chat_session.send_message(prompt)\n",
        "        return response\n",
        "    response = call_api_with_backoff(api_call)\n",
        "    return response.text\n",
        "\n",
        "\n",
        "def invoke_ollama(model_name, prompt):\n",
        "    url = \"http://localhost:11434/api/chat\"\n",
        "    command_dict = {\n",
        "        \"model\": model_name,\n",
        "        \"stream\": False,\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    try:\n",
        "        command_json = json.dumps(command_dict)\n",
        "        prompt_invoke = requests.post(\n",
        "            url, data=command_json, headers={\"Content-Type\": \"application/json\"}\n",
        "        )\n",
        "        prompt_invoke.raise_for_status()\n",
        "        json_res = prompt_invoke.json()\n",
        "        if 'message' in json_res and 'content' in json_res['message']:\n",
        "            response = json_res['message']['content']\n",
        "        else:\n",
        "            print(\"Error: The response does not contain the 'message' or 'content' key.\")\n",
        "            response = None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Connection error: {e}\")\n",
        "        response = None\n",
        "        json_res = None\n",
        "    return json_res, response\n",
        "\n",
        "\n",
        "def process_all_jailbreak_prompts(input_base_dir, output_base_dir, models, categories):\n",
        "    for model in models:\n",
        "        model_name = model.upper()\n",
        "\n",
        "        for task in os.listdir(input_base_dir):\n",
        "            if task.startswith('.'):\n",
        "                continue  # Exclude hidden files/directories\n",
        "            task_path = os.path.join(input_base_dir, task)\n",
        "            if not os.path.isdir(task_path):\n",
        "                continue\n",
        "\n",
        "            for attack_name in os.listdir(task_path):\n",
        "                if attack_name.startswith('.'):\n",
        "                    continue\n",
        "                attack_path = os.path.join(task_path, attack_name)\n",
        "                if not os.path.isdir(attack_path):\n",
        "                    continue\n",
        "\n",
        "                excel_files = [\n",
        "                    f for f in os.listdir(attack_path)\n",
        "                    if not f.startswith('.') and (f.endswith('.xlsx') or f.endswith('.xls'))\n",
        "                ]\n",
        "                if not excel_files:\n",
        "                    print(f\"No Excel file found in {attack_path}\")\n",
        "                    continue\n",
        "\n",
        "                for excel_file in excel_files:\n",
        "                    file_path = os.path.join(attack_path, excel_file)\n",
        "                    excel_base = os.path.splitext(excel_file)[0]\n",
        "                    try:\n",
        "                        df = pd.read_excel(file_path)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error reading {file_path}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                    prompts_data = []\n",
        "                    for _, row in df.iterrows():\n",
        "                        prompt = row[\"PROMPT\"]\n",
        "                        bias_category = row[\"BIAS CATEGORY\"] if \"BIAS CATEGORY\" in row else \"\"\n",
        "                        prompts_data.append({\n",
        "                            \"PROMPT\": prompt,\n",
        "                            \"BIAS CATEGORY\": bias_category\n",
        "                        })\n",
        "\n",
        "                    out_dir = os.path.join(output_base_dir, model_name, task, attack_name)\n",
        "                    os.makedirs(out_dir, exist_ok=True)\n",
        "                    results = []\n",
        "\n",
        "                    for prompt_data in prompts_data:\n",
        "                        prompt = prompt_data[\"PROMPT\"]\n",
        "                        bias_category = prompt_data[\"BIAS CATEGORY\"]\n",
        "                        if bias_category.upper() in categories:\n",
        "                            continue\n",
        "                        if model.lower() in [\"{your_ollama_models}\"]:\n",
        "                            _, response = invoke_ollama(model, prompt)\n",
        "                        elif model.lower() == \"deepseek\":\n",
        "                            response = invoke_ds(prompt)\n",
        "                        elif model.lower() == \"gemini\":\n",
        "                             response = invoke_gemini(prompt)\n",
        "                        else:\n",
        "                            response = invoke_db(model, prompt)\n",
        "\n",
        "                        result = {\n",
        "                            \"MODEL\": model_name,\n",
        "                            \"BIAS CATEGORY\": bias_category,\n",
        "                            \"PROMPT\": prompt,\n",
        "                            \"RESPONSE\": response,\n",
        "                        }\n",
        "                        results.append(result)\n",
        "\n",
        "                    if results:\n",
        "                        output_file = os.path.join(out_dir, f\"{model_name}_{excel_base}.csv\")\n",
        "                        df_out = pd.DataFrame(results)\n",
        "                        df_out.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
        "                        print(f\"Saved {output_file} with {len(results)} responses\")\n",
        "\n",
        "\n",
        "def process_all_base_prompts(input_base_dir, output_base_dir, models):\n",
        "    for model in models:\n",
        "        model_name = model.upper()\n",
        "\n",
        "        for file in os.listdir(input_base_dir):\n",
        "            if file.startswith('.') or not (file.endswith('.xlsx') or file.endswith('.xls')):\n",
        "                continue\n",
        "\n",
        "            file_path = os.path.join(input_base_dir, file)\n",
        "            excel_base = os.path.splitext(file)[0]\n",
        "\n",
        "            try:\n",
        "                df = pd.read_excel(file_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {file_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "            prompts_data = []\n",
        "            for _, row in df.iterrows():\n",
        "                prompt = row[\"PROMPT\"]\n",
        "                bias_category = row[\"BIAS CATEGORY\"] if \"BIAS CATEGORY\" in row else \"\"\n",
        "                prompts_data.append({\n",
        "                    \"PROMPT\": prompt,\n",
        "                    \"BIAS CATEGORY\": bias_category\n",
        "                })\n",
        "\n",
        "            out_dir = os.path.join(output_base_dir, model_name)\n",
        "            os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "            results = []\n",
        "            for prompt_data in prompts_data:\n",
        "                prompt = prompt_data[\"PROMPT\"]\n",
        "                bias_category = prompt_data[\"BIAS CATEGORY\"]\n",
        "\n",
        "                if model.lower() in [\"{your_ollama_models}\"]:\n",
        "                    _, response = invoke_ollama(model, prompt)\n",
        "                elif model.lower() == \"deepseek\":\n",
        "                    response = invoke_ds(prompt)\n",
        "                elif model.lower() == \"gemini\":\n",
        "                    response = invoke_gemini(prompt)\n",
        "                else:\n",
        "                    response = invoke_db(model, prompt)\n",
        "\n",
        "                result = {\n",
        "                    \"MODEL\": model_name,\n",
        "                    \"BIAS CATEGORY\": bias_category,\n",
        "                    \"PROMPT\": prompt,\n",
        "                    \"RESPONSE\": response,\n",
        "                }\n",
        "                results.append(result)\n",
        "\n",
        "            if results:\n",
        "                output_file = os.path.join(out_dir, f\"{model_name}_{excel_base}.csv\")\n",
        "                df_out = pd.DataFrame(results)\n",
        "                df_out.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
        "                print(f\"Saved {output_file} with {len(results)} responses\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# ======== Base prompts =========\n",
        "# ===============================\n",
        "\n",
        "models = ['{your_models}']\n",
        "\n",
        "print(\"Started\")\n",
        "start_time = time.time()\n",
        "process_all_base_prompts(\"prompts\", \"results_base\", models)\n",
        "end_time = time.time()\n",
        "total_time = (end_time - start_time) / 60\n",
        "hours = total_time // 60\n",
        "minutes = total_time % 60\n",
        "print(f\"Total time: {hours} hours and {minutes} minutes\")"
      ],
      "metadata": {
        "id": "4vJQa435yDZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# ====== Jailbreak prompts ======\n",
        "# ===============================\n",
        "\n",
        "bias_category_no_attack = {\"{model}\": ['{bias_categories}']}\n",
        "\n",
        "print(\"Started\")\n",
        "start_time = time.time()\n",
        "for model in bias_category_no_attack:\n",
        "    process_all_jailbreak_prompts(\"prompts\", \"results_jailbreak\", models=[model], categories=bias_category_no_attack[model])\n",
        "end_time = time.time()\n",
        "total_time = (end_time - start_time) / 60\n",
        "hours = total_time // 60\n",
        "minutes = total_time % 60\n",
        "print(f\"Total time: {hours} hours and {minutes} minutes\")"
      ],
      "metadata": {
        "id": "Az7OKAySuuYT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}